{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e68b3487-a778-4919-8930-21d2d3122ddb",
   "metadata": {},
   "source": [
    "Demonstration/proof-of-concept code to take the output of the eScriptorium_chars notebook and then run a basic classifier on the results for a given letter.\n",
    "Assumes the following:\n",
    "- A folder `~/Image_data/escr_chars/{doc_id}`, where `doc_id` can be specified below but is intended to be the id (pk) of a given document on an instance of eScriptorium. This is intended to be the output of the eScriptorium_chars notebook but does not need to be.\n",
    "- Subfolders named for the different characters, containing images of instances of the characters\n",
    "- The image sizes are assumed to be approximately 100x100 pixels. If they are very different then you may want to change the size of the VGG input layer accordingly (specified in `vgg_inputsize` below).\n",
    "\n",
    "Based heavily on demonstration code by Chahan Vidal-Gorene provided in the context of a course on Computational Palaeography jointly taught by the two of us in the Master Humanités numériques at Université PSL.\n",
    "\n",
    "Peter Stokes, EPHE-PSL, March 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c71180-e915-47b6-a427-936b36dcb554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.utils import disable_interactive_logging, enable_interactive_logging\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a362415-902d-458c-9ab5-b37e21a57cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_inputsize = (112,112)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008df2e-735e-435e-bdb3-9225cf79dc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_images(folder_path, target_size=vgg_inputsize):\n",
    "    model = VGG16(weights='imagenet', include_top=False)\n",
    "    feature_list = []\n",
    "    image_paths = []\n",
    "    \n",
    "    for file in tqdm(os.listdir(folder_path), desc=\"Preprocessing images\"):\n",
    "        img_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            img = image.load_img(img_path, target_size=target_size)\n",
    "            img_array = image.img_to_array(img)\n",
    "            img_array = np.expand_dims(img_array, axis=0)\n",
    "            img_array = preprocess_input(img_array)\n",
    "            \n",
    "            features = model.predict(img_array).flatten()\n",
    "            feature_list.append(features)\n",
    "            image_paths.append(img_path)\n",
    "        except Exception as e:\n",
    "            # Faulty image; just ignore for now\n",
    "            print(f\"Error for image {file}: {e}\")\n",
    "            \n",
    "    return np.array(feature_list), image_paths\n",
    "\n",
    "def reduce_dimensionality(features, n_components=50):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    features_pca = pca.fit_transform(features)\n",
    "    return features_pca, pca\n",
    "\n",
    "def cluster_images(features_pca, method='kmeans', n_clusters=5):\n",
    "    if method == 'kmeans':\n",
    "        model = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    elif method == 'dbscan':\n",
    "        model = DBSCAN(eps=5, min_samples=2)\n",
    "    \n",
    "    labels = model.fit_predict(features_pca)\n",
    "    return labels\n",
    "\n",
    "def visualize_clusters(image_paths, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    for label in unique_labels:\n",
    "        idxs = np.where(labels == label)[0]\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        for i, idx in enumerate(idxs[:60]):  # Show max 10 images per cluster\n",
    "            img = cv2.imread(image_paths[idx])\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            plt.subplot(5, 12, i + 1)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(f'Cluster {label}')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dceac4e-c2c7-43db-8f5b-d730d0a1f149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default interactive logging displays one status bar per image which is much more annoying than helpful\n",
    "#doc_id = 4982\n",
    "doc_id = 2917\n",
    "char = 'a'\n",
    "\n",
    "folder_path = os.path.join(os.path.expanduser('~'), 'Image_data', 'escr_chars', str(doc_id), char)\n",
    "\n",
    "disable_interactive_logging()\n",
    "features, image_paths = load_and_preprocess_images(folder_path)\n",
    "enable_interactive_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c078fe-b8f9-43ca-b20d-c115a127607b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pca, pca = reduce_dimensionality(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5741ead7-2411-49f2-80b9-94edec13ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = cluster_images(features_pca, method='kmeans', n_clusters=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66445f-a43f-490e-aaaf-60822e217e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_clusters(image_paths, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b057805-cc8d-420d-b2e5-2c5d358095ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_feature_map(image_path, target_size=vgg_inputsize):\n",
    "    model = VGG16(weights='imagenet', include_top=False)\n",
    "    \n",
    "    img = image.load_img(image_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = preprocess_input(img_array)\n",
    "    \n",
    "    features = model.predict(img_array)\n",
    "    heatmap = np.mean(features[0], axis=-1)\n",
    "    heatmap = cv2.resize(heatmap, (target_size[1], target_size[0]))\n",
    "    heatmap = np.uint8(255 * heatmap / np.max(heatmap))\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    \n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\n",
    "    \n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis('off')\n",
    "    plt.title(\"Feature Activation Map\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4d1a2-ca09-4310-afff-ae13314c7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_feature_map(os.path.join(folder_path, 'char-2917-545034-0.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
