{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924470d6-7fe1-4ea1-8f17-8745e74286b1",
   "metadata": {},
   "source": [
    "Demonstration/proof-of-concept code to demonstrate the use of the [kraken OCR/HTR software](https://kraken.re/) via its API.\n",
    "This notebook follows the [kraken tutorial](https://kraken.re/main/api.html) closely.\n",
    "\n",
    "Peter Stokes, EPHE-PSL, March 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1a08e-4002-47f2-b73a-0ae3af74b966",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "First, we set up the relevant libraries and create a couple of generic helper functions.\n",
    "\n",
    "First, we need to install Kraken in our Collab environment so that we can use it. **If you are doing this on a system with kraken already installed (e.g. your own computer) then you should skip this step.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca915b2-e4c7-4295-8ede-4eca88052b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only if kraken not already installed (e.g. in Google Collab)\n",
    "# Kraken pinned to 5.2.9 because later versions seem to conflict with the Collab setup, but this is likely to change.\n",
    "\n",
    "#!pip install kraken==5.2.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cc051c-4136-4694-97c4-625e52cbcbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kraken\n",
    "from kraken import blla, serialization\n",
    "from kraken.lib import vgsl\n",
    "from PIL import Image\n",
    "\n",
    "import io, urllib, json, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6992f06-89a2-43f9-beb5-9efcbaa65053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a sample image to test. Note that we don't need a very high resolution image, so let's be good citizens and\n",
    "# download a version reduced to 25% (note pct:25 in the URL).\n",
    "# For further information see the IIIF Image API: https://iiif.io/api/image/3.0/#4-image-requests\n",
    "\n",
    "img_url = 'https://iiif.bodleian.ox.ac.uk/iiif/image/671d12e9-e014-417d-bba1-c3f16ff447f1/full/pct:25/0/default.jpg'\n",
    "\n",
    "fd = urllib.request.urlopen(img_url)\n",
    "image_file = io.BytesIO(fd.read())\n",
    "im = Image.open(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36926510-1fe4-43ea-8001-ddf6c9ab5447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic function to download a file and store locally\n",
    "# TODO: if file already exists then could simply exit, or have a flag to replace or not\n",
    "\n",
    "def download_file(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        with open(filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                file.write(chunk)\n",
    "        print(f\"File downloaded successfully to {filename}\")\n",
    "        return filename\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f228df-f367-47b5-b18f-15e009a3f753",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "Let's try downloading some freely-available segmentation models from the Zenodo repository and GitHub and see how they differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e9d6b0-d0c5-4321-a79d-f32e91f0e2df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Segment using the default blla model for comparison and have a look at the resulting data structure\n",
    "\n",
    "baseline_seg = blla.segment(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43564c34-38f4-49c8-9591-ca7da3873e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many regions and lines are detected\n",
    "\n",
    "print(len(baseline_seg.regions), \"regions detected\")\n",
    "print(len(baseline_seg.lines), \"lines detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5d580-b102-4b04-ba39-873180a00e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try some more specialised models. This one is designed to find interlinear glosses\n",
    "\n",
    "interlinear_url = 'https://github.com/malamatenia/Eutyches/raw/refs/heads/main/kraken-YALTAi/models/interlinear_BL.mlmodel'\n",
    "interlinear_path = download_file(url=interlinear_url, filename='interlinear_BL.mlmodel')\n",
    "interlinear_model = vgsl.TorchVGSLModel.load_model(interlinear_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0094312-cc51-47c2-beaa-c5d9d9cc0f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the segmentation and see how many interlinear additions it found\n",
    "\n",
    "interlinear_seg = blla.segment(im, model = interlinear_model)\n",
    "print(len(interlinear_seg.lines), \"interlinear additions detected on this page\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f05bc6-c7e2-4c48-8ff5-c32351428eba",
   "metadata": {},
   "source": [
    "# Recognition\n",
    "\n",
    "Here we download some models for recognition (automatic transcription) and test them on another image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1ad3a0-0ece-430b-be82-c937bc468119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an appropriate model. If you change the test image then be sure to change the model if necessary.\n",
    "from kraken.lib import models\n",
    "\n",
    "recmodel_url = 'https://zenodo.org/records/15030337/files/catmus-medieval-1.6.0.mlmodel?download=1'\n",
    "recmodel_path = download_file(url=recmodel_url, filename='catmus-medieval-1.6.0.mlmodel')\n",
    "recmodel = models.load_any(recmodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106a660a-937d-4c7d-b390-a43ec01f7cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a new image...\n",
    "\n",
    "#img_url = 'https://bl.digirati.io/images/ark:/81055/vdc_100059910515.0x00006d/full/pct:25/0/default.jpg'\n",
    "img_url = 'https://stacks.stanford.edu/image/iiif/pg511wq8230%252F520_034_R_TC_46/full/pct:25/0/default.jpg'\n",
    "\n",
    "# NB this is a large image even at 25%, so be patient!\n",
    "#img_url = 'https://iiif.durham.ac.uk/iiif/trifle/32150/t2/mc/z3/t2mcz30ps641/f7a5ce05416d134803625dcdddc84339.jp2/full/pct:25/0/default.jpg'\n",
    "\n",
    "fd = urllib.request.urlopen(img_url)\n",
    "image_file = io.BytesIO(fd.read())\n",
    "im = Image.open(image_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041cc228-fdf3-4a96-a87e-318b352861d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now segment it...\n",
    "\n",
    "baseline_seg = blla.segment(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f96fb1-561e-45bc-9597-60d84f08b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the recognition, given the recognition model and the results of our segmentation\n",
    "\n",
    "from kraken.rpred import rpred\n",
    "\n",
    "pred_it = rpred(network=recmodel,\n",
    "                    im=im,\n",
    "                    bounds=baseline_seg)\n",
    "\n",
    "# Print the raw transcription\n",
    "for record in pred_it:\n",
    "    print(record)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cd3c2-460d-43af-bb5a-56189b324a3c",
   "metadata": {},
   "source": [
    "Note the data structure of the prediction results. From the kraken tutorial:\n",
    "\n",
    "> The output isnâ€™t just a sequence of characters but, depending on the type of segmentation supplied, a kraken.containers.BaselineOCRRecord or kraken.containers.BBoxOCRRecord record object containing the character prediction, cuts (approximate locations), and confidences.\n",
    "\n",
    "Let's have a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993af494-7c88-48eb-b8b9-d318d60cb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "record.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d8441-8cd0-4532-8375-bbfc671beb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "record.confidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcfce42-1b95-4f05-8789-1868553b6039",
   "metadata": {},
   "outputs": [],
   "source": [
    "record.cuts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d34378-8b88-4202-9a99-9abe5982b861",
   "metadata": {},
   "source": [
    "# Palaeographical analysis using Cuts\n",
    "\n",
    "Although kraken is designed for transcription, it does give approximate information about the likely location of characters on the image. This isn't perfect, but we can use it to automatically show the images. Because this is only an approximation, we can increase the size of the image in order to increase the likelihood of capturing the full letter. As you will see, this does not work particularly well, but we will see a slightly more sophisticated approach in the next workbook.\n",
    "\n",
    "We could create images, but since we're working with IIIF, let's instead generate the IIIF URL to each image. This means that we also need to convert the kraken coordinates to the format that IIIF expects. We also need to take into account that we have scaled the image by 25%, but according to IIIF the scaling happens *after* the region is calculated, so we need to multiply our coordinates by 4 to allow for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a18d27f-99c8-4392-9949-112acc15c1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we are just using the cuts variable directly, which means we will be looking at the last-detected line on the page.\n",
    "\n",
    "x_marg = 30 # Add a margin of error\n",
    "y_marg = 10 # Usually the vertical is fairly correct (at least for this type of script.\n",
    "char_urls = []\n",
    "\n",
    "for c in record.cuts:\n",
    "    xy1, xy2, xy3, xy4 = c\n",
    "    start_x = xy1[0] - x_marg\n",
    "    start_y = xy1[1] - y_marg\n",
    "    end_x = xy3[0] - xy1[0] + x_marg\n",
    "    end_y = xy3[1] - xy1[1] + y_marg\n",
    "\n",
    "    char_urls.append(img_url.replace(\"/full/pct:25/0\", f\"/{start_x*4},{start_y*4},{end_x*4},{end_y*4}/full/0\"))\n",
    "\n",
    "print(char_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b69bc0-cb68-40d6-af7c-f84e8227ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_char = 'e'\n",
    "\n",
    "a_indexes = [i for i, x in enumerate(record.prediction) if x == search_char]\n",
    "print(a_indexes)\n",
    "print([char_urls[i+1] for i in a_indexes])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
